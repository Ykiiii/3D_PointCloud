{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICP 点云精配准\n",
    "本教程演示了ICP（迭代最接近点）配准算法。多年来，它一直是研究和工业领域中几何配准的主流。\n",
    "输入是两个点云和一个初始变换，该变换将源点云与目标点云大致对齐。输出是一个细化的变换，将两个点云紧密地对齐。\n",
    "在配准过程中，一个辅助函数draw_registration_result将对齐情况可视化。\n",
    "在本教程中，我们展示了两个ICP变体，点对点ICP和点对平面ICP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import open3d_example as o3de\n",
    "import numpy as np\n",
    "import copy,time,re,os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助的可视化函数\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 1])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp],\n",
    "                                      zoom=0.4459,\n",
    "                                      front=[0.9288, -0.2951, -0.2242],\n",
    "                                      lookat=[1.6784, 2.0612, 1.4451],\n",
    "                                      up=[-0.3402, -0.9189, -0.1996])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 制作算法输入\n",
    "# 从两个文件中读取一个源点云和一个目标点云。给出了一个粗略的变换。\n",
    "# 最初的对齐方式通常是通过全局配准算法得到的。例子见全局配准。\n",
    "\n",
    "source = o3d.io.read_point_cloud(\"PointCloudData\\\\DemoICPPointClouds\\\\cloud_bin_0.pcd\")\n",
    "target = o3d.io.read_point_cloud(\"PointCloudData\\\\DemoICPPointClouds\\\\cloud_bin_1.pcd\")\n",
    "threshold = 0.02\n",
    "trans_init = np.asarray([[0.862, 0.011, -0.507, 0.5],\n",
    "                         [-0.139, 0.967, -0.215, 0.7],\n",
    "                         [0.487, 0.255, 0.835, -1.4], \n",
    "                         [0.0, 0.0, 0.0, 1.0]])\n",
    "draw_registration_result(source, target, trans_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数 evaluate_registration 计算两个主要指标。\n",
    "# fitness，衡量重叠的区域(# inlier correspondences / # of points in target)。越高越好。\n",
    "# inlier_rmse, 衡量所有inlier对应点的RMSE。越低越好。\n",
    "print(\"Initial alignment\")\n",
    "evaluation = o3d.pipelines.registration.evaluate_registration(\n",
    "  source, target, threshold, trans_init)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 点对点 ICP\n",
    "一般来说，ICP算法经过两步迭代。\n",
    "\n",
    "1. 从目标点云P和用当前变换矩阵T变换的源点云Q中找到对应集K={(p,q)}。\n",
    "2. 通过最小化定义在对应集K上的目标函数E(T)来更新变换T。\n",
    "\n",
    "不同的ICP变体使用不同的目标函数E（T）\n",
    "\n",
    "TransformationEstimationPointToPoint类提供了计算点对点ICP目标的残差和雅各布矩阵的函数。函数registration_icp将其作为参数，并运行点对点ICP来获得结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Apply point-to-point ICP\")\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    source, target, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "print(reg_p2p)\n",
    "print(\"Transformation is:\")\n",
    "print(reg_p2p.transformation)\n",
    "draw_registration_result(source, target, reg_p2p.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认情况下，registration_icp运行到收敛或达到最大迭代次数（默认为30次）。\n",
    "# 可以改变它以允许更多的计算时间并进一步改善结果。\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    source, target, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "    o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=2000))\n",
    "print(reg_p2p)\n",
    "print(\"Transformation is:\")\n",
    "print(reg_p2p.transformation)\n",
    "draw_registration_result(source, target, reg_p2p.transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 点对面 ICP\n",
    "点对平面的ICP算法[ChenAndMedioni1992]使用一个不同的目标函数\n",
    "\n",
    "其中np是p点的法线。[Rusinkiewicz2001]表明，点到平面的ICP算法比点到点的ICP算法具有更快的收敛速度。\n",
    "\n",
    "registration_icp被调用时有一个不同的参数TransformationEstimationPointToPlane。在内部，这个类实现了计算点对平面ICP目标的残差和雅各布矩阵的函数。\n",
    "\n",
    "**点对平面的ICP算法使用点法线。在本教程中，我们从文件中加载法线。如果没有给出法线，可以用顶点法线估计来计算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Apply point-to-plane ICP\")\n",
    "reg_p2l = o3d.pipelines.registration.registration_icp(\n",
    "    source, target, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "print(reg_p2l)\n",
    "print(\"Transformation is:\")\n",
    "print(reg_p2l.transformation)\n",
    "draw_registration_result(source, target, reg_p2l.transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 鲁棒核ICP\n",
    "本教程演示了鲁棒核在剔除异常值方面的使用。在这个特定的教程中，我们将使用ICP（迭代最接近点）配准算法作为我们要处理异常值的目标问题。即便如此，该理论也适用于任何特定的优化问题，而不仅仅是ICP。目前，鲁棒的核子只在PointToPlane ICP中实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "初始对齐通常由全局配准算法获得。例子见全局配准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp],\n",
    "                                      zoom=0.4459,\n",
    "                                      front=[0.9288, -0.2951, -0.2242],\n",
    "                                      lookat=[1.6784, 2.0612, 1.4451],\n",
    "                                      up=[-0.3402, -0.9189, -0.1996])\n",
    "source = o3d.io.read_point_cloud(\"PointCloudData\\\\DemoICPPointClouds\\\\cloud_bin_0.pcd\")\n",
    "target = o3d.io.read_point_cloud(\"PointCloudData\\\\DemoICPPointClouds\\\\cloud_bin_1.pcd\")\n",
    "# threshold = 0.2\n",
    "trans_init = np.asarray([[0.862, 0.011, -0.507, 0.5],\n",
    "                         [-0.139, 0.967, -0.215, 0.7],\n",
    "                         [0.487, 0.255, 0.835, -1.4], [0.0, 0.0, 0.0, 1.0]])\n",
    "draw_registration_result(source, target, trans_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了更好地展示在配准中使用鲁棒核的优势，我们在源点云中加入一些人工生成的高斯噪声。\n",
    "def apply_noise(pcd, mu, sigma):\n",
    "    noisy_pcd = copy.deepcopy(pcd)\n",
    "    points = np.asarray(noisy_pcd.points)\n",
    "    points += np.random.normal(mu, sigma, size=points.shape)\n",
    "    noisy_pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    return noisy_pcd\n",
    "\n",
    "\n",
    "mu, sigma = 0, 0.1  # mean and standard deviation\n",
    "source_noisy = apply_noise(source, mu, sigma)\n",
    "\n",
    "print(\"Source PointCloud + noise:\")\n",
    "o3d.visualization.draw_geometries([source_noisy],\n",
    "                                  zoom=0.4459,\n",
    "                                  front=[0.353, -0.469, -0.809],\n",
    "                                  lookat=[2.343, 2.217, 1.809],\n",
    "                                  up=[-0.097, -0.879, 0.467])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 普通icp对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 普通icp\n",
    "threshold = 0.02\n",
    "print(\"Vanilla point-to-plane ICP, threshold={}:\".format(threshold))\n",
    "p2l = o3d.pipelines.registration.TransformationEstimationPointToPlane()\n",
    "reg_p2l = o3d.pipelines.registration.registration_icp(source_noisy, target,\n",
    "                                                      threshold, trans_init,\n",
    "                                                      p2l)\n",
    "\n",
    "print(reg_p2l)\n",
    "print(\"Transformation is:\")\n",
    "print(reg_p2l.transformation)\n",
    "draw_registration_result(source, target, reg_p2l.transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整普通ICP\n",
    "# 鉴于我们现在处理的是高斯噪声，我们可以尝试增加阈值来搜索最近的邻居，以改善配准结果。\n",
    "# 我们可以看到，在这些条件下，如果没有一个健壮的内核，传统的ICP没有机会处理离群值\n",
    "threshold = 1.0\n",
    "print(\"Vanilla point-to-plane ICP, threshold={}:\".format(threshold))\n",
    "p2l = o3d.pipelines.registration.TransformationEstimationPointToPlane()\n",
    "reg_p2l = o3d.pipelines.registration.registration_icp(source_noisy, target,\n",
    "                                                      threshold, trans_init,\n",
    "                                                      p2l)\n",
    "\n",
    "print(reg_p2l)\n",
    "print(\"Transformation is:\")\n",
    "print(reg_p2l.transformation)\n",
    "draw_registration_result(source, target, reg_p2l.transformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 鲁棒核ICP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 鲁棒核ICP\n",
    "# 使用相同的阈值=1.0和一个稳健的内核，我们可以正确地配准两个点云。\n",
    "threshold = 1\n",
    "print(\"Robust point-to-plane ICP, threshold={}:\".format(threshold))\n",
    "loss = o3d.pipelines.registration.TukeyLoss(k=sigma)\n",
    "print(\"Using robust loss:\", loss)\n",
    "p2l = o3d.pipelines.registration.TransformationEstimationPointToPlane(loss)\n",
    "reg_p2l = o3d.pipelines.registration.registration_icp(source_noisy, target,\n",
    "                                                      threshold, trans_init,\n",
    "                                                      p2l)\n",
    "print(reg_p2l)\n",
    "print(\"Transformation is:\")\n",
    "print(reg_p2l.transformation)\n",
    "draw_registration_result(source, target, reg_p2l.transformation)\n",
    "\n",
    "# 对于参数k，我们将其设置为与噪声模型的标准偏差k=σ相匹配。\n",
    "# Robust Kernels中使用的参数k通常被选为与输入数据的噪声模型的标准差相匹配。\n",
    "# 在这个意义上，k是区分离群者/离群者的工具。\n",
    "# 尽管这在现实世界的数据中并不总是那么容易定义，但对于合成数据来说，为了说明鲁棒核的好处，这很容易固定下来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 彩色点云配准\n",
    "本教程演示了一个ICP变体，它同时使用几何和颜色进行配准。它实现了[Park2017]的算法。颜色信息锁定了沿切线平面的对齐。因此，这个算法比之前的点云配准算法更准确、更稳健，而运行速度与ICP配准的速度相当。本教程使用ICP配准的符号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_registration_result_original_color(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target],\n",
    "                                      zoom=0.5,\n",
    "                                      front=[-0.2458, -0.8088, 0.5342],\n",
    "                                      lookat=[1.7745, 2.2305, 0.9787],\n",
    "                                      up=[0.3109, -0.5878, -0.7468])\n",
    "print(\"1. Load two point clouds and show initial pose\")\n",
    "# demo_colored_icp_pcds = o3d.data.DemoColoredICPPointClouds()\n",
    "# source = o3d.io.read_point_cloud(demo_colored_icp_pcds.paths[0])\n",
    "# target = o3d.io.read_point_cloud(demo_colored_icp_pcds.paths[1])\n",
    "\n",
    "source = o3d.io.read_point_cloud(\"PointCloudData\\\\DemoICPPointClouds\\\\cloud_bin_0.pcd\")\n",
    "target = o3d.io.read_point_cloud(\"PointCloudData\\\\DemoICPPointClouds\\\\cloud_bin_1.pcd\")\n",
    "# draw initial alignment\n",
    "current_transformation = np.identity(4)\n",
    "draw_registration_result_original_color(source, target, current_transformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 点对面的ICP(作对比，与上同)\n",
    "首先运行点对平面ICP作为一个基准方法。下面的可视化图显示了错位的绿色三角形纹理。这是因为几何约束并不能阻止两个平面的滑移。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point to plane ICP\n",
    "current_transformation = np.identity(4)# 这里更改了初始矩阵，导致效果差\n",
    "print(\"2. Point-to-plane ICP registration is applied on original point\")\n",
    "print(\"   clouds to refine the alignment. Distance threshold 0.02.\")\n",
    "result_icp = o3d.pipelines.registration.registration_icp(\n",
    "    source, target, 0.02, current_transformation,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "print(result_icp)\n",
    "draw_registration_result_original_color(source, target,\n",
    "                                        result_icp.transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 彩色点云配准\n",
    "（也不是很准）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colored pointcloud registration\n",
    "# This is implementation of following paper\n",
    "# J. Park, Q.-Y. Zhou, V. Koltun,\n",
    "# Colored Point Cloud Registration Revisited, ICCV 2017\n",
    "voxel_radius = [0.04, 0.02, 0.01]\n",
    "max_iter = [50, 30, 14]\n",
    "current_transformation = np.identity(4)\n",
    "print(\"3. Colored point cloud registration\")\n",
    "for scale in range(3):\n",
    "    iter = max_iter[scale]\n",
    "    radius = voxel_radius[scale]\n",
    "    print([iter, radius, scale])\n",
    "\n",
    "    print(\"3-1. Downsample with a voxel size %.2f\" % radius)\n",
    "    source_down = source.voxel_down_sample(radius)\n",
    "    target_down = target.voxel_down_sample(radius)\n",
    "\n",
    "    print(\"3-2. Estimate normal.\")\n",
    "    source_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "    target_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "\n",
    "    print(\"3-3. Applying colored point cloud registration\")\n",
    "    result_icp = o3d.pipelines.registration.registration_colored_icp(\n",
    "        source_down, target_down, radius, current_transformation,\n",
    "        o3d.pipelines.registration.TransformationEstimationForColoredICP(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=1e-6,\n",
    "                                                          relative_rmse=1e-6,\n",
    "                                                          max_iteration=iter))\n",
    "    current_transformation = result_icp.transformation\n",
    "    print(result_icp)\n",
    "draw_registration_result_original_color(source, target,\n",
    "                                        result_icp.transformation)\n",
    "# 总共有3层多分辨率的点云是用voxel_down_sample创建的。\n",
    "# 法线是通过顶点法线估计来计算的。\n",
    "# lambda_geometric是registration_colored_icp的一个可选参数，它决定了λ∈[0,1]的整体能量λEG+(1-λ)EC。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全局配准Global registration\n",
    "ICP配准和彩色点云配准都被称为局部配准方法，因为它们依赖于粗略的对齐作为初始化。本教程展示了另一类配准方法，被称为全局配准。这个系列的算法不需要对齐的初始化。它们通常产生不太严格的对齐结果，并被用作局部方法的初始化\n",
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化，这个辅助函数将转换后的源点云与目标点云一起可视化。\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp],\n",
    "                                      zoom=0.4559,\n",
    "                                      front=[0.6452, -0.3036, -0.7011],\n",
    "                                      lookat=[1.9892, 2.0208, 1.8945],\n",
    "                                      up=[-0.2779, -0.9482, 0.1556])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取几何特征\n",
    "# 对点云进行降样，估计法线，然后为每个点计算一个FPFH特征。\n",
    "# FPFH特征是一个33维的向量，描述了一个点的局部几何属性。\n",
    "# 33维空间中的近邻查询可以返回具有相似的局部几何结构的点\n",
    "def preprocess_point_cloud(pcd, voxel_size):\n",
    "    print(\":: Downsample with a voxel size %.3f.\" % voxel_size)\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    radius_normal = voxel_size * 2\n",
    "    print(\":: Estimate normal with search radius %.3f.\" % radius_normal)\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "    radius_feature = voxel_size * 5\n",
    "    print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    return pcd_down, pcd_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "# 下面的代码从两个文件中读取源点云和目标点云。它们与一个单位矩阵错位作为变换。\n",
    "def prepare_dataset(voxel_size):\n",
    "    print(\":: Load two point clouds and disturb initial pose.\")\n",
    "\n",
    "    # demo_icp_pcds = o3d.data.DemoICPPointClouds()\n",
    "    # source = o3d.io.read_point_cloud(demo_icp_pcds.paths[0])\n",
    "    # target = o3d.io.read_point_cloud(demo_icp_pcds.paths[1])\n",
    "    source = o3d.io.read_point_cloud(\"PointCloudData\\\\DemoICPPointClouds\\\\cloud_bin_0.pcd\")\n",
    "    target = o3d.io.read_point_cloud(\"PointCloudData\\\\DemoICPPointClouds\\\\cloud_bin_1.pcd\")\n",
    "\n",
    "    trans_init = np.asarray([[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n",
    "    source.transform(trans_init)\n",
    "    draw_registration_result(source, target, np.identity(4))\n",
    "\n",
    "    source_down, source_fpfh = preprocess_point_cloud(source, voxel_size)\n",
    "    target_down, target_fpfh = preprocess_point_cloud(target, voxel_size)\n",
    "    return source, target, source_down, target_down, source_fpfh, target_fpfh\n",
    "voxel_size = 0.05  # means 5cm for this dataset\n",
    "source, target, source_down, target_down, source_fpfh, target_fpfh = prepare_dataset(\n",
    "    voxel_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ransac迭代全局配准\n",
    "我们使用RANSAC进行全局配准。在每个RANSAC迭代中，从源点云中挑选ransac_n个随机点。它们在目标点云中的对应点是通过查询33维FPFH特征空间中的最近邻居来检测的。剪枝步骤采取快速剪枝算法，以快速拒绝早期的错误匹配。\n",
    "\n",
    "Open3D提供了以下剪枝算法:\n",
    "\n",
    "- CorrespondenceCheckerBasedOnDistance检查对齐的点云是否接近（小于指定阈值）。\n",
    "- CorrespondenceCheckerBasedOnEdgeLength检查从源和目标对应关系中单独绘制的任何两条任意边（由两个顶点形成的线）的长度是否相似。本教程检查||edgesource||>0.9⋅||edgetarget||和||edgetarget||>0.9⋅|edgesource||为真。\n",
    "- CorrespondenceCheckerBasedOnNormal考虑任何对应关系的顶点法线亲和力。它计算两个法线向量的点积。它采用一个弧度值作为阈值。 ` 只有通过剪枝步骤的匹配才会被用来计算转换，并在整个点云上进行验证。核心功能是基于特征匹配的配准登记\n",
    "  \n",
    "只有通过剪枝步骤的匹配才用于计算转换，并在整个点云上进行验证。核心函数是registration_ransac_based_on_feature_matching。这个函数最重要的超参数是RANSACConvergenceCriteria。它定义了RANSAC迭代的最大次数和置信概率。这两个数字越大，表示结果越准确，但也意味着算法花费的时间越长。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_global_registration(source_down, target_down, source_fpfh,\n",
    "                                target_fpfh, voxel_size):\n",
    "    distance_threshold = voxel_size * 1.5\n",
    "    print(\":: RANSAC registration on downsampled point clouds.\")\n",
    "    print(\"   Since the downsampling voxel size is %.3f,\" % voxel_size)\n",
    "    print(\"   we use a liberal distance threshold %.3f.\" % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "        distance_threshold,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "        3, [\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(\n",
    "                0.9),\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(\n",
    "                distance_threshold)\n",
    "        ], o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 0.999))\n",
    "    return result\n",
    "result_ransac = execute_global_registration(source_down, target_down,\n",
    "                                            source_fpfh, target_fpfh,\n",
    "                                            voxel_size)\n",
    "print(result_ransac)\n",
    "draw_registration_result(source_down, target_down, result_ransac.transformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 点对面ICP局部细化\n",
    "由于性能的原因，全局配准只在严重下采样的点云上进行。其结果也是不紧密的。我们使用点对平面的ICP来进一步细化配准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_registration(source, target, source_fpfh, target_fpfh, voxel_size):\n",
    "    distance_threshold = voxel_size * 0.4\n",
    "    print(\":: Point-to-plane ICP registration is applied on original point\")\n",
    "    print(\"   clouds to refine the alignment. This time we use a strict\")\n",
    "    print(\"   distance threshold %.3f.\" % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, distance_threshold, result_ransac.transformation,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "    return result\n",
    "result_icp = refine_registration(source, target, source_fpfh, target_fpfh,\n",
    "                                 voxel_size)\n",
    "print(result_icp)\n",
    "draw_registration_result(source, target, result_icp.transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速全局配准\n",
    "由于无数的模型建议和评估，基于RANSAC的全局配准解决方案可能需要很长的时间。[Zhou2016]引入了一种更快的方法，快速优化少数对应关系的线程权重。由于每次迭代都不涉及模型建议和评估，[Zhou2016]中提出的方法可以节省大量的计算时间。\n",
    "\n",
    "本教程将基于RANSAC的全局配准的运行时间与[Zhou2016]的实现进行了比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用与上面的全局配准例子相同的输入\n",
    "voxel_size = 0.05  # means 5cm for the dataset\n",
    "source, target, source_down, target_down, source_fpfh, target_fpfh = \\\n",
    "        prepare_dataset(voxel_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ransac迭代全局配准的耗时\n",
    "start = time.time()\n",
    "result_ransac = execute_global_registration(source_down, target_down,\n",
    "                                            source_fpfh, target_fpfh,\n",
    "                                            voxel_size)\n",
    "print(\"Global registration took %.3f sec.\\n\" % (time.time() - start))\n",
    "print(result_ransac)\n",
    "draw_registration_result(source_down, target_down, result_ransac.transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速全局配准\n",
    "def execute_fast_global_registration(source_down, target_down, source_fpfh,\n",
    "                                     target_fpfh, voxel_size):\n",
    "    distance_threshold = voxel_size * 0.5\n",
    "    print(\":: Apply fast global registration with distance threshold %.3f\" \\\n",
    "            % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_fgr_based_on_feature_matching(\n",
    "        source_down, target_down, source_fpfh, target_fpfh,\n",
    "        o3d.pipelines.registration.FastGlobalRegistrationOption(\n",
    "            maximum_correspondence_distance=distance_threshold))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速全局配准的耗时\n",
    "start = time.time()\n",
    "result_fast = execute_fast_global_registration(source_down, target_down,\n",
    "                                               source_fpfh, target_fpfh,\n",
    "                                               voxel_size)\n",
    "print(\"Fast global registration took %.3f sec.\\n\" % (time.time() - start))\n",
    "print(result_fast)\n",
    "draw_registration_result(source_down, target_down, result_fast.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGR快速全局配准\n",
    "# 除了基于fpfh特征的FGR之外，还可以通过registration_fgr_based_on_correspondence使用基于通信的FGR执行全局配准。\n",
    "# 如果您的通信前端与FPFH不同，则此方法很有用，但您仍然希望在给定一组假定通信的情况下使用FGR。它可以被调用\n",
    "def execute_fast_global_registration(source_down, target_down, \n",
    "                                     correspondence_set, voxel_size):\n",
    "    distance_threshold = voxel_size * 0.5\n",
    "    print(\":: Apply fast global registration with distance threshold %.3f\" \\\n",
    "            % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_fgr_based_on_correspondence(\n",
    "        source_down, target_down, correspondence_set,\n",
    "        o3d.pipelines.registration.FastGlobalRegistrationOption(\n",
    "            maximum_correspondence_distance=distance_threshold))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多向配准Multiway registration\n",
    "多向配准是指在一个全局空间中对多个几何体进行配准的过程。通常输入是一组几何体（例如，点云或RGBD图像）{Pi}。输出是一组刚性变换{Ti}，从而使变换后的点云{TiPi}在全局空间中保持一致。\n",
    "\n",
    "Open3D通过姿态图优化实现了多向配准。后台实现了[Choi2015]中提出的技术。\n",
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件中读取三个点云。这些点云被降低采样率，并一起可视化。它们是错位的\n",
    "def load_point_clouds(voxel_size=0.0):\n",
    "    pcds = []\n",
    "    DataFolder = \"PointCloudData\\\\DemoICPPointClouds\"\n",
    "    for path in os.listdir(DataFolder):\n",
    "        pcd = o3d.io.read_point_cloud(DataFolder+\"\\\\\"+path)\n",
    "        pcd_down = pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "        pcds.append(pcd_down)\n",
    "    return pcds\n",
    "voxel_size = 0.02\n",
    "pcds_down = load_point_clouds(voxel_size)\n",
    "o3d.visualization.draw_geometries(pcds_down,\n",
    "                                  zoom=0.3412,\n",
    "                                  front=[0.4257, -0.2125, -0.8795],\n",
    "                                  lookat=[2.6172, 2.0475, 1.532],\n",
    "                                  up=[-0.0694, -0.9768, 0.2024])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配准成位姿图\n",
    "\n",
    "位姿图有两个关键元素：节点和边。一个节点是一块与位姿矩阵Ti相关的几何体Pi，它将Pi转换到全局空间。集合{Ti}是要优化的未知变量。PoseGraph.nodes是一个PoseGraphNode的列表。我们设定全局空间为P0的空间。因此T0是身份矩阵。其他的位姿矩阵是通过累积相邻节点之间的变换来初始化的。相邻的节点通常有很大的重叠，可以用点对平面的ICP进行配准。\n",
    "\n",
    "位姿图的边连接着两个重叠的节点（几何体的碎片）。每条边都包含一个变换矩阵Ti,j，将源几何体Pi与目标几何体Pj对齐。本教程使用点对平面ICP来估计变换。在更复杂的情况下，这个成对的配准问题应该通过全局配准来解决。\n",
    "\n",
    "[Choi2015]已经观察到成对配准是容易出错的。错误的成对配准可能会超过正确配准的对数。因此，他们将位姿图的边缘划分为两类。位姿图边连接着时间上接近的、相邻的节点。像ICP这样的局部配准算法可以可靠地对齐它们。环形闭合边连接任何非相邻的节点。对齐是通过全局配准找到的，可靠性较低。在Open3D中，这两类边缘是由PoseGraphEdge初始化器中的不确定参数来区分的。\n",
    "\n",
    "除了变换矩阵Ti之外，用户还可以为每条边设置一个信息矩阵Λi。如果Λi是用函数get_information_matrix_from_point_clouds设置的，该位姿图边缘的损失近似于两个节点之间相应集合的RMSE，有一个线程权重。详情请参考[Choi2015]中的公式（3）到（9）以及Redwood配准基准。\n",
    "\n",
    "该脚本创建了一个有三个节点和三条边的位姿图。在这些边中，有两条是测距边（uncertain = False），一条是循环闭合边（uncertain = True）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_registration(source, target):\n",
    "    print(\"Apply point-to-plane ICP\")\n",
    "    icp_coarse = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, max_correspondence_distance_coarse, np.identity(4),\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "    icp_fine = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, max_correspondence_distance_fine,\n",
    "        icp_coarse.transformation,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "    transformation_icp = icp_fine.transformation\n",
    "    information_icp = o3d.pipelines.registration.get_information_matrix_from_point_clouds(\n",
    "        source, target, max_correspondence_distance_fine,\n",
    "        icp_fine.transformation)\n",
    "    return transformation_icp, information_icp\n",
    "\n",
    "\n",
    "def full_registration(pcds, max_correspondence_distance_coarse,\n",
    "                      max_correspondence_distance_fine):\n",
    "    pose_graph = o3d.pipelines.registration.PoseGraph()\n",
    "    odometry = np.identity(4)\n",
    "    pose_graph.nodes.append(o3d.pipelines.registration.PoseGraphNode(odometry))\n",
    "    n_pcds = len(pcds)\n",
    "    for source_id in range(n_pcds):\n",
    "        for target_id in range(source_id + 1, n_pcds):\n",
    "            transformation_icp, information_icp = pairwise_registration(\n",
    "                pcds[source_id], pcds[target_id])\n",
    "            print(\"Build o3d.pipelines.registration.PoseGraph\")\n",
    "            if target_id == source_id + 1:  # odometry case\n",
    "                odometry = np.dot(transformation_icp, odometry)\n",
    "                pose_graph.nodes.append(\n",
    "                    o3d.pipelines.registration.PoseGraphNode(\n",
    "                        np.linalg.inv(odometry)))\n",
    "                pose_graph.edges.append(\n",
    "                    o3d.pipelines.registration.PoseGraphEdge(source_id,\n",
    "                                                             target_id,\n",
    "                                                             transformation_icp,\n",
    "                                                             information_icp,\n",
    "                                                             uncertain=False))\n",
    "            else:  # loop closure case\n",
    "                pose_graph.edges.append(\n",
    "                    o3d.pipelines.registration.PoseGraphEdge(source_id,\n",
    "                                                             target_id,\n",
    "                                                             transformation_icp,\n",
    "                                                             information_icp,\n",
    "                                                             uncertain=True))\n",
    "    return pose_graph\n",
    "print(\"Full registration ...\")\n",
    "max_correspondence_distance_coarse = voxel_size * 15\n",
    "max_correspondence_distance_fine = voxel_size * 1.5\n",
    "with o3d.utility.VerbosityContextManager(\n",
    "        o3d.utility.VerbosityLevel.Debug) as cm:\n",
    "    pose_graph = full_registration(pcds_down,\n",
    "                                   max_correspondence_distance_coarse,\n",
    "                                   max_correspondence_distance_fine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizing PoseGraph ...\")\n",
    "option = o3d.pipelines.registration.GlobalOptimizationOption(\n",
    "    max_correspondence_distance=max_correspondence_distance_fine,\n",
    "    edge_prune_threshold=0.25,\n",
    "    reference_node=0)\n",
    "with o3d.utility.VerbosityContextManager(\n",
    "        o3d.utility.VerbosityLevel.Debug) as cm:\n",
    "    o3d.pipelines.registration.global_optimization(\n",
    "        pose_graph,\n",
    "        o3d.pipelines.registration.GlobalOptimizationLevenbergMarquardt(),\n",
    "        o3d.pipelines.registration.GlobalOptimizationConvergenceCriteria(),\n",
    "        option)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全局优化在位姿图上执行两次。第一遍对原始位姿图的位姿进行优化，考虑到所有的边，并尽力区分不确定的边之间的错误排列。这些错误的排列具有较小的线程权重，它们在第一遍之后被修剪掉。第二遍运行时不考虑它们，并产生一个紧密的全局对齐。在这个例子中，所有的边都被认为是真正的对齐，因此第二遍立即终止。\n",
    "\n",
    "## 可视化优化\n",
    "\n",
    "转换后的点云被列出，并使用draw_geometries进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transform points and display\")\n",
    "for point_id in range(len(pcds_down)):\n",
    "    print(pose_graph.nodes[point_id].pose)\n",
    "    pcds_down[point_id].transform(pose_graph.nodes[point_id].pose)\n",
    "o3d.visualization.draw_geometries(pcds_down,\n",
    "                                  zoom=0.3412,\n",
    "                                  front=[0.4257, -0.2125, -0.8795],\n",
    "                                  lookat=[2.6172, 2.0475, 1.532],\n",
    "                                  up=[-0.0694, -0.9768, 0.2024])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并点云\n",
    "PointCloud有一个方便的操作符+，可以将两个点云合并成一个。在下面的代码中，合并后的点使用voxel_down_sample进行了统一的重采样。这是建议在合并点云后进行的后处理，因为它可以缓解重复的或过度密集的点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcds = load_point_clouds(voxel_size)\n",
    "pcd_combined = o3d.geometry.PointCloud()\n",
    "for point_id in range(len(pcds)):\n",
    "    pcds[point_id].transform(pose_graph.nodes[point_id].pose)\n",
    "    pcd_combined += pcds[point_id]\n",
    "pcd_combined_down = pcd_combined.voxel_down_sample(voxel_size=voxel_size)\n",
    "o3d.io.write_point_cloud(\"test\\\\multiway_registration.pcd\", pcd_combined_down)\n",
    "o3d.visualization.draw_geometries([pcd_combined_down],\n",
    "                                  zoom=0.3412,\n",
    "                                  front=[0.4257, -0.2125, -0.8795],\n",
    "                                  lookat=[2.6172, 2.0475, 1.532],\n",
    "                                  up=[-0.0694, -0.9768, 0.2024])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGBD集成，从RGBD重建mesh\n",
    "Open3D实现了一个可扩展的RGBD图像整合算法。该算法是基于[Curless1996]和[Newcombe2011]中提出的技术。为了支持大型场景，我们在ElasticReconstruction中使用了Integrater中引入的分层散列结构。\n",
    "\n",
    "## 从.log文件中读取轨迹\n",
    "\n",
    "本教程使用函数read_trajectory从.log文件中读取摄像机的轨迹。一个.log文件的样本如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraPose:\n",
    "\n",
    "    def __init__(self, meta, mat):\n",
    "        self.metadata = meta\n",
    "        self.pose = mat\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Metadata : ' + ' '.join(map(str, self.metadata)) + '\\n' + \\\n",
    "            \"Pose : \" + \"\\n\" + np.array_str(self.pose)\n",
    "\n",
    "\n",
    "def read_trajectory(filename):\n",
    "    traj = []\n",
    "    with open(filename, 'r') as f:\n",
    "        metastr = f.readline()\n",
    "        while metastr:\n",
    "            metadata = list(map(int, metastr.split()))\n",
    "            mat = np.zeros(shape=(4, 4))\n",
    "            for i in range(4):\n",
    "                matstr = f.readline()\n",
    "                mat[i, :] = np.fromstring(matstr, dtype=float, sep=' \\t')\n",
    "            traj.append(CameraPose(metadata, mat))\n",
    "            metastr = f.readline()\n",
    "    return traj\n",
    "# redwood_rgbd = o3d.data.SampleRedwoodRGBDImages()\n",
    "camera_poses = read_trajectory(\"RGBD_Data\\\\SampleRedwoodRGBDImages\\\\trajectory.log\")\n",
    "print(camera_poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSDF体积的集成\n",
    "\n",
    "Open3D提供两种类型的TSDF体积。UniformTSDFVolume和ScalableTSDFVolume。推荐使用后者，因为它使用分层结构，从而支持更大的场景。\n",
    "\n",
    "voxel_length = 4.0 / 512.0意味着TSDF体积的单个体素尺寸是4.0m/512.0=7.8125mm。降低这个值可以得到高分辨率的TSDF体积，但积分结果容易受到深度噪声的影响。 sdf_trunc = 0.04 指定有符号距离函数（SDF）的截断值。当color_type = TSDFVolumeColorType.RGB8时，8位RGB颜色也被整合为TSDF体积的一部分。当color_type = TSDFVolumeColorType.Gray32和convert_rgb_to_intensity = True时，浮点数类型的强度可以被整合。颜色整合的灵感来自于PCL。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = o3d.pipelines.integration.ScalableTSDFVolume(\n",
    "    voxel_length=4.0 / 512.0,\n",
    "    sdf_trunc=0.04,\n",
    "    color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8)\n",
    "\n",
    "\n",
    "DataFolder = \"RGBD_Data/SampleRedwoodRGBDImages\"\n",
    "redwood_rgbd_color_paths = [DataFolder+\"/color/\"+os.path.relpath(i) for i in os.listdir(DataFolder+\"/color\")]\n",
    "redwood_rgbd_depth_paths = [DataFolder+\"/depth/\"+os.path.relpath(i) for i in os.listdir(DataFolder+\"/depth\")]\n",
    "for i in range(len(camera_poses)):\n",
    "    print(\"Integrate {:d}-th image into the volume.\".format(i))\n",
    "    color = o3d.io.read_image(redwood_rgbd_color_paths[i])\n",
    "    depth = o3d.io.read_image(redwood_rgbd_depth_paths[i])\n",
    "    rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        color, depth, depth_trunc=4.0, convert_rgb_to_intensity=False)\n",
    "    volume.integrate(\n",
    "        rgbd,\n",
    "        o3d.camera.PinholeCameraIntrinsic(\n",
    "            o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault),\n",
    "        np.linalg.inv(camera_poses[i].pose))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取mesh\n",
    "网格提取采用行进立方体算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extract a triangle mesh from the volume and visualize it.\")\n",
    "mesh = volume.extract_triangle_mesh()\n",
    "mesh.compute_vertex_normals()\n",
    "# mesh.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "o3d.visualization.draw_geometries([mesh],\n",
    "                                  front=[0.9292, 0.3531, -0.1083],\n",
    "                                  lookat=[-1.9787, 0.5713, 2.5180],\n",
    "                                  up=[-0.3502, 0.9356, 0.0460],\n",
    "                                  zoom=0.56)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGBD里程计\n",
    "一个RGBD里程计可以找到两个连续的RGBD图像对之间的相机运动。输入是RGBDImage的两个实例。输出是以刚体变换的形式出现的运动。Open3D实现了[Steinbrucker2011]和[Park2017]的方法。\n",
    "## 读取相机内参\n",
    "我们首先从一个json文件中读取相机的内在矩阵\n",
    "\n",
    "注意: Open3D中的很多小数据结构都可以从json文件中读取/写入。这包括相机内在属性、相机轨迹、位姿图等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[525.    0.  319.5]\n",
      " [  0.  525.  239.5]\n",
      " [  0.    0.    1. ]]\n"
     ]
    }
   ],
   "source": [
    "pinhole_camera_intrinsic = o3d.io.read_pinhole_camera_intrinsic(\n",
    "    \"RGBD_Data\\\\SampleRedwoodRGBDImages\\\\camera_primesense.json\")\n",
    "print(pinhole_camera_intrinsic.intrinsic_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取RGBD图像\n",
    "\n",
    "这个代码块读取了两对Redwood格式的RGBD图像。我们参考Redwood数据集以获得全面的解释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFolder = \"RGBD_Data/SampleRedwoodRGBDImages\"\n",
    "redwood_rgbd_color_paths = [DataFolder+\"/color/\"+os.path.relpath(i) for i in os.listdir(DataFolder+\"/color\")]\n",
    "redwood_rgbd_depth_paths = [DataFolder+\"/depth/\"+os.path.relpath(i) for i in os.listdir(DataFolder+\"/depth\")]\n",
    "\n",
    "source_color = o3d.io.read_image(redwood_rgbd_color_paths[0])\n",
    "source_depth = o3d.io.read_image(redwood_rgbd_depth_paths[0])\n",
    "target_color = o3d.io.read_image(redwood_rgbd_color_paths[1])\n",
    "target_depth = o3d.io.read_image(redwood_rgbd_depth_paths[1])\n",
    "source_rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    source_color, source_depth)\n",
    "target_rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    target_color, target_depth)\n",
    "target_pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "    target_rgbd_image, pinhole_camera_intrinsic)\n",
    "# 注意: Open3D假定彩色图像和深度图像是同步的，并登记在同一坐标框架内。这通常可以通过打开RGBD相机设置中的同步和注册功能来实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从两个RGBD图像对中计算里程数\n",
    "这个代码块调用两个不同的RGBD测距方法。第一个是来自[Steinbrucker2011]。它最小化了对齐图像的照片一致性。第二个是来自[Park2017]。除了照片一致性之外，它还实现了对几何的约束。两个函数的运行速度相似，但在我们对基准数据集的测试中，[Park2017]更准确，因此是推荐的方法。\n",
    "\n",
    "OdometryOption()中有几个参数:\n",
    "\n",
    "- minimum_correspondence_ratio。对齐后，测量两个RGBD图像的重叠率。如果两幅RGBD图像的重叠区域小于指定的比率，那么dometry模块就认为这是一个失败的案例。\n",
    "\n",
    "- max_depth_diff: 在深度图像领域，如果两个对齐的像素的深度差小于指定的值，它们就被认为是一个对应关系。较大的值会引起更积极的搜索，但容易产生不稳定的结果。\n",
    "\n",
    "- min_depth和max_depth。小于或大于指定深度值的像素被忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OdometryOption class.\n",
      "iteration_number_per_pyramid_level = [ 20, 10, 5, ] \n",
      "depth_diff_max = 0.030000\n",
      "depth_min = 0.000000\n",
      "depth_max = 4.000000\n"
     ]
    }
   ],
   "source": [
    "option = o3d.pipelines.odometry.OdometryOption()\n",
    "odo_init = np.identity(4)\n",
    "print(option)\n",
    "# 两种方法\n",
    "[success_color_term, trans_color_term,\n",
    " info] = o3d.pipelines.odometry.compute_rgbd_odometry(\n",
    "     source_rgbd_image, target_rgbd_image, pinhole_camera_intrinsic, odo_init,\n",
    "     o3d.pipelines.odometry.RGBDOdometryJacobianFromColorTerm(), option)\n",
    "[success_hybrid_term, trans_hybrid_term,\n",
    " info] = o3d.pipelines.odometry.compute_rgbd_odometry(\n",
    "     source_rgbd_image, target_rgbd_image, pinhole_camera_intrinsic, odo_init,\n",
    "     o3d.pipelines.odometry.RGBDOdometryJacobianFromHybridTerm(), option)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGBD图像对的可视化\n",
    "\n",
    "RGBD图像对被转换为点云并一起渲染。请注意，代表第一幅（源）RGBD图像的点云被转换为由里程计估计的变换。在这个转换之后，两个点云都被对齐了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RGB-D Odometry\n",
      "[[ 9.99988286e-01 -7.53983409e-05 -4.83963172e-03  2.74054550e-04]\n",
      " [ 1.83909052e-05  9.99930634e-01 -1.17782559e-02  2.29634918e-02]\n",
      " [ 4.84018408e-03  1.17780289e-02  9.99918922e-01  6.02121265e-04]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: 不支持请求的转换操作。 \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: 不支持请求的转换操作。 \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: 句柄无效。 \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: 不支持请求的转换操作。 \n",
      "Using Hybrid RGB-D Odometry\n",
      "[[ 9.99992973e-01 -2.51084541e-04 -3.74035273e-03 -1.07049775e-03]\n",
      " [ 2.07046059e-04  9.99930714e-01 -1.17696227e-02  2.32280983e-02]\n",
      " [ 3.74304875e-03  1.17687656e-02  9.99923740e-01  1.40592054e-03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 两种算法的结果对比\n",
    "if success_color_term:\n",
    "    print(\"Using RGB-D Odometry\")\n",
    "    print(trans_color_term)\n",
    "    source_pcd_color_term = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "        source_rgbd_image, pinhole_camera_intrinsic)\n",
    "    source_pcd_color_term.transform(trans_color_term)\n",
    "    o3d.visualization.draw_geometries([target_pcd, source_pcd_color_term],\n",
    "                                      zoom=0.48,\n",
    "                                      front=[0.0999, -0.1787, -0.9788],\n",
    "                                      lookat=[0.0345, -0.0937, 1.8033],\n",
    "                                      up=[-0.0067, -0.9838, 0.1790])\n",
    "if success_hybrid_term:\n",
    "    print(\"Using Hybrid RGB-D Odometry\")\n",
    "    print(trans_hybrid_term)\n",
    "    source_pcd_hybrid_term = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "        source_rgbd_image, pinhole_camera_intrinsic)\n",
    "    source_pcd_hybrid_term.transform(trans_hybrid_term)\n",
    "    o3d.visualization.draw_geometries([target_pcd, source_pcd_hybrid_term],\n",
    "                                      zoom=0.48,\n",
    "                                      front=[0.0999, -0.1787, -0.9788],\n",
    "                                      lookat=[0.0345, -0.0937, 1.8033],\n",
    "                                      up=[-0.0067, -0.9838, 0.1790])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 颜色映射优化\n",
    "考虑将颜色映射到从深度摄像机重建的几何体上。由于颜色和深度帧不是完全一致的，使用颜色图像的纹理映射会导致颜色图的模糊。Open3D提供了由[Zhou2014]提出的颜色映射优化方法。下面的脚本显示了一个颜色贴图优化的例子。\n",
    "## 初始化\n",
    "下面的代码读取颜色和深度图像对，并制作rgbd_image。注意，convert_rgb_to_intensity标志是False。这是为了保留8位颜色通道，而不是使用单通道浮点型图像。\n",
    "\n",
    "在将RGBD图像应用于颜色地图优化之前，将其可视化始终是一个好的做法。debug_mode开关可以被设置为True，以实现RGBD图像的可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] Read geometry::Image failed: file extension ds_store unknown\n",
      "[Open3D WARNING] Read geometry::Image failed: file extension ds_store unknown\n",
      "[Open3D WARNING] Read JPG failed: unable to open file: RGBD_Data\\SampleFountainRGBDImages/color/0000010-000001228920.jpg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[Open3D Error] (class std::shared_ptr<class open3d::geometry::RGBDImage> __cdecl open3d::geometry::RGBDImage::CreateFromColorAndDepth(const class open3d::geometry::Image &,const class open3d::geometry::Image &,double,double,bool)) D:\\a\\Open3D\\Open3D\\cpp\\open3d\\geometry\\RGBDImageFactory.cpp:21: Unsupported image format.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15468\\2598976351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmesh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgbd_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcamera_trajectory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Load dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmesh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgbd_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcamera_trajectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_fountain_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15468\\2598976351.py\u001b[0m in \u001b[0;36mload_fountain_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo3d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfountain_rgbd_dataset_color_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n\u001b[1;32m---> 10\u001b[1;33m             color, depth)\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mrgbd_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgbd_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [Open3D Error] (class std::shared_ptr<class open3d::geometry::RGBDImage> __cdecl open3d::geometry::RGBDImage::CreateFromColorAndDepth(const class open3d::geometry::Image &,const class open3d::geometry::Image &,double,double,bool)) D:\\a\\Open3D\\Open3D\\cpp\\open3d\\geometry\\RGBDImageFactory.cpp:21: Unsupported image format.\n"
     ]
    }
   ],
   "source": [
    "def load_fountain_dataset():\n",
    "    rgbd_images = []\n",
    "    DataFolder = \"RGBD_Data\\\\SampleFountainRGBDImages\"\n",
    "    fountain_rgbd_dataset_color_paths = [DataFolder+\"/color/\"+os.path.relpath(i) for i in os.listdir(DataFolder+\"/image\")]\n",
    "    fountain_rgbd_dataset_depth_paths = [DataFolder+\"/depth/\"+os.path.relpath(i) for i in os.listdir(DataFolder+\"/depth\")]\n",
    "    for i in range(len(fountain_rgbd_dataset_depth_paths)):\n",
    "        depth = o3d.io.read_image(fountain_rgbd_dataset_depth_paths[i])\n",
    "        color = o3d.io.read_image(fountain_rgbd_dataset_color_paths[i])\n",
    "        rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            color, depth, convert_rgb_to_intensity=False)\n",
    "        rgbd_images.append(rgbd_image)\n",
    "\n",
    "    camera_trajectory = o3d.io.read_pinhole_camera_trajectory(\n",
    "        \"RGBD_Data\\SampleFountainRGBDImages\\scene\\key.log\")\n",
    "    mesh = o3d.io.read_triangle_mesh(\n",
    "        \"RGBD_Data\\SampleFountainRGBDImages\\scene\\integrated.ply\")\n",
    "\n",
    "    return mesh, rgbd_images, camera_trajectory\n",
    "# Load dataset\n",
    "mesh, rgbd_images, camera_trajectory = load_fountain_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] Read JPG failed: unable to open file: RGBD_Data\\SampleFountainRGBDImages\\color\\0000010-000001228920.jpg\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[Open3D Error] (class std::shared_ptr<class open3d::geometry::RGBDImage> __cdecl open3d::geometry::RGBDImage::CreateFromColorAndDepth(const class open3d::geometry::Image &,const class open3d::geometry::Image &,double,double,bool)) D:\\a\\Open3D\\Open3D\\cpp\\open3d\\geometry\\RGBDImageFactory.cpp:21: Unsupported image format.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15468\\111080107.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo3d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfountain_rgbd_dataset_color_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mrgbd_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo3d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRGBDImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_from_color_and_depth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;31m# rgbd_images.append(rgbd_image)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [Open3D Error] (class std::shared_ptr<class open3d::geometry::RGBDImage> __cdecl open3d::geometry::RGBDImage::CreateFromColorAndDepth(const class open3d::geometry::Image &,const class open3d::geometry::Image &,double,double,bool)) D:\\a\\Open3D\\Open3D\\cpp\\open3d\\geometry\\RGBDImageFactory.cpp:21: Unsupported image format.\n"
     ]
    }
   ],
   "source": [
    "rgbd_images = []\n",
    "DataFolder = \"RGBD_Data\\\\SampleFountainRGBDImages\"\n",
    "fountain_rgbd_dataset_color_paths = [DataFolder+\"\\\\color\\\\\"+os.path.relpath(i) for i in os.listdir(DataFolder+\"\\\\image\")]\n",
    "fountain_rgbd_dataset_depth_paths = [DataFolder+\"\\\\depth\\\\\"+os.path.relpath(i) for i in os.listdir(DataFolder+\"\\\\depth\")]\n",
    "for i in range(len(fountain_rgbd_dataset_depth_paths)):\n",
    "    depth = o3d.io.read_image(fountain_rgbd_dataset_depth_paths[i])\n",
    "    color = o3d.io.read_image(fountain_rgbd_dataset_color_paths[i])\n",
    "    print(i)\n",
    "    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(color, depth)\n",
    "    # rgbd_images.append(rgbd_image)\n",
    "\n",
    "# camera_trajectory = o3d.io.read_pinhole_camera_trajectory(\n",
    "#     \"RGBD_Data\\SampleFountainRGBDImages\\scene\\key.log\")\n",
    "# mesh = o3d.io.read_triangle_mesh(\n",
    "#     \"RGBD_Data\\SampleFountainRGBDImages\\scene\\integrated.ply\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000010-000001228920.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000031-000004096400.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000044-000005871507.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000064-000008602440.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000110-000014883587.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000156-000021164733.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000200-000027172787.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000215-000029220987.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000255-000034682853.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000299-000040690907.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000331-000045060400.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000368-000050112627.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000412-000056120680.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000429-000058441973.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000474-000064586573.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000487-000066361680.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000526-000071687000.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000549-000074827573.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000582-000079333613.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000630-000085887853.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000655-000089301520.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000703-000095855760.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000722-000098450147.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000771-000105140933.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000792-000108008413.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000818-000111558627.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000849-000115791573.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000883-000120434160.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000896-000122209267.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000935-000127534587.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0000985-000134361920.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0001028-000140233427.jpg',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\color\\\\0001061-000144739467.jpg']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(fountain_rgbd_dataset_color_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0000038-000001234662.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0000124-000004104418.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0000177-000005872988.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0000259-000008609267.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0000447-000014882686.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0000635-000021156105.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0000815-000027162570.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0000877-000029231463.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0001040-000034670651.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0001220-000040677116.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0001351-000045048488.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0001503-000050120614.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0001683-000056127079.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0001752-000058429557.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0001937-000064602868.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0001990-000066371438.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0002149-000071677149.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0002243-000074813859.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0002378-000079318707.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0002575-000085892450.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0002677-000089296113.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0002874-000095869855.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0002951-000098439288.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0003152-000105146507.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0003238-000108016262.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0003344-000111553403.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0003471-000115791298.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0003610-000120429623.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0003663-000122198194.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0003823-000127537274.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0004028-000134377970.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0004203-000140217589.png',\n",
       " 'RGBD_Data\\\\SampleFountainRGBDImages\\\\depth\\\\0004339-000144755807.png']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(fountain_rgbd_dataset_depth_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
